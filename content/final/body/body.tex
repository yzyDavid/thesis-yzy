\cleardoublestylepage{common}

\section{正文}

\subsection{目标}

本部分描述本设计所开发的系统的功能与目的。

\subsubsection{背景与趋势}

\subsubsection{问题与解决方案}

\subsection{开发技术}

本部分列出工程实现上所依赖的全部技术。

golang v1.11.5

GoLand 2019.1.1

Kubernetes v1.14

minikube 1.0.0

operator-framework v1.0.0

\subsection{工程背景}

本部分将会论述为了实现本系统，依赖的开源组件与开源解决方案等。

\subsubsection{Kubernetes}

Kubernetes （下文中可能简称为k8s）是由云原生基金会所维护的项目，其设计思想脱胎于 Google 内部使用的 Borg 集群管理系统。Kubernetes 借助于容器的实现，负责容器编排与资源调度分配。Kubernetes 的概念与所包含的组件在下文中详细介绍，通过这段说明可以使读者更容易理解如此一个容器编排系统负责了什么工作，解决了什么问题。

\subsubsection{Tensorflow Serving}

Tensorflow 是近年来生态最完善的深度学习框架之一，从其工作方式来看，也可以被称为计算图框架。其深度学习相关的部分我们不在这里做过多讨论，有很丰富的资料，感兴趣的读者可以自行查阅。这里我们介绍一下 Tensorflow Serving，这个负责使用训练好的模型的组件，其工作方式，与容器生态的关系，最终得出为何用它作为本设计中的代表应用的例子。

Tensorflow Serving 可以将 Tensorflow 训练得到的模型，注意默认支持 saved model 格式的模型，对 tensorflow 产出训练结果有了解的读者应当知道其中差别。

用户运行 Tensorflow Serving 时，通过命令行参数或者环境变量等等方式传递模型的名称，路径等参数，不需要修改源代码或者编写更多的代码就可以使模型上线服务，这样可以做到模型数据和逻辑的分离，对方便使用，加快部署迭代速度有帮助。

Tensorflow Serving 非常注重面向容器的生态，不难得出结论，只需提供模型的数据文件，就可以被加载服务，这说明 Tensorflow Serving 属于带数据的只读服务。结果上，Tensorflow Serving 官方将 docker image 作为比二进制更重要的发布渠道，用户通过 docker registry pull 合适版本的 Serving 镜像，就可以直接使用，无疑是更方便运维，对比与使用的。这也是我选择使用 Tensorflow Serving 作为示例应用，进行对它的调度算法实验，开发的重要原因。另一个重要原因是，作为机器学习服务，它属于业务逻辑和框架本身界限明显，明确区分，分开理解的典型代表。如果从数据/业务与框架分离的角度考虑，任何一个区分租户的数据库或搜索引擎系统也可以作为例子，但是这里他们的代表性不如机器学习服务明显，且机器学习作为相对而言的新兴事物，对它的尝试和探索更有价值。但是这不代表本设计的成果就对其他领域没有意义了，这一点还需要读者明辨。

\subsubsection{Kubernetes Operator}

\subsection{实现方式}

本部分将会详细讲解本人实现此系统的详细过程。

\subsubsection{搭建 Kubernetes 开发环境}

作为一个生产级别的容器编排系统，天然就是面向分布式系统，保证可靠性的，工作方式也需要天然面向分布式系统，为部署在多台机器上做设计。同时此系统将应用程序的运行，甚至网络环境都进行了接管，因此对运行环境有很强的依赖。实际操作上也是如此，我调研了多种搭建 Kubernetes 环境的方案，最终只有一种方式实现起来相对简单。

\subsection{算法描述}

本部分讨论在不涉及系统实现的细节前提下，调度算法的背景，意义与实现。

\subsubsection{背景}

本部分讨论一个通用的调度算法需要考虑哪些内容。

我们先思考一下，一个通用的应用程序运行需要依赖哪些资源。资源总共分为软件资源和硬件资源两类。软件资源包括操作系统版本，动态链接库依赖等，这里不在我们的主要讨论范围之内，暂时不考虑。硬件资源包括计算资源，即处理器时间；存储资源，包括主存储器，外置存储器如硬盘容量；网络带宽；异构计算设备，包括，型号，类型，使用率等等限制。

这里定义我们的调度算法为，在软件资源依赖能满足的前提下，将不同的多租户应用部分编排至不同的应用与机器，并保证满足各个租户业务与数据的资源声明得到满足。

为了满足这些资源限定，一个通用的算法需要对可能出现的所有资源类型进行分类，才能从更抽象的角度进行考虑。因为我们无法枚举可能出现的一切资源类型。

一切资源限度要求，都可以用一个标量来描述。如 CPU 计算资源需求，可以将单位核数输出的计算力定义为一个定值，假设就是 100，那么每个应用对 CPU 计算能力的要求就可以使用一个数值来描述了。抽象来讲，即是一个标量加上任意自定义的单位即可描述。

然而考虑到计算资源横向扩展的要求，资源的种类又需要做出区分。考虑内存消耗，对于深度学习模型而言，内存消耗主要是模型加载至内存中，每一台运行服务的服务器都需要消耗等量的内存。再考虑计算资源消耗，对于稳定相同的计算任务或者流量，消耗的总计算力之和不变，横向扩展后新增的服务器不需要额外浪费计算力，而是负担了原有服务器不能承担的一部分任务。我们将前一种横向扩展后每台服务器都要消耗等量的资源称为硬性资源，后一种可以被横向扩展利用的资源称为弹性资源。在绝大多数场景下，如此划分资源就可以描述弹性调度场景下资源的消耗量。

到此为止，我们已经得到了一个通用调度算法的定义，即具有多种资源约束，资源约束都被归类至硬性资源和弹性资源其中之一，同时满足。下文我们开始讨论常用的优化算法，并着重考虑他们在我们场景下的使用方式。

\subsubsection{分析}

在这个背景下，调度算法可以处理如下问题：

算法的输入如下：

有 $N$ 个业务，或称租户，模型，下面统称模型，记为 $Model_1,Model_2,...,Model_N$。每个模型都需要 $a$ 个硬性资源 $Rfix_1,Rfix_2,...,Rfix_a$，$b$ 个弹性资源 $Relastic_1,Relastic_2,...,Relastic_b$。我们还有 $M$ 种机器类型，每种机器类型也需要给出上述硬性资源和弹性资源的提供量 $Rfix_1,Rfix_2,...,Rfix_a,Relastic_1,Relastic_2,...,Relastic_b$。

算法的输出如下：

我们需要给出 $d$ 个部署信息，记为 $Deployment_1,Deployment_2,...,Deployment_d$，每个部署信息包括了其所包含的模型列表以及需要的机器个数，即模型列表和副本数组成的二元组：$([Model_x,Model_y,...,Model_{last}],Replicas)$。

\subsubsection{贪心算法}

贪心算法是一类算法思想的统称，指每一步都选择当前一步考虑范围内的最优解，最终得到的解就是全剧最优解的一类算法。如果放宽条件，广义的贪心算法也就指每一步都选择考虑范围内的最优解的算法。因为在足够复杂的场景内，我们不一定能给出一个调度问题的全局最优解，我们这里的所采用的贪心策略，将使用后一种广义的定义。

在我们的场景下，这个优化问题即是一个这样的组合问题，将模型组合于不同的部署规格之中。

\subsubsection{背包问题}

所谓背包问题，指的是我们具有多个背包，每个背包能承载一定量的重量，背包当中需要存放重量各自不同的物品，我们以一个确定的目标，如使用给定的背包数目和规格装下的物品重量之和最大。

对于调度算法来讲，不考虑动态改变配置的场景下调度动作路径的问题，则可以简化为如下一个问题：

对于异质数据和计算逻辑的部署，我们称呼一份数据和计算模型一致的多副本为一个应用(application)，承载多个相同应用的一组进程副本为集群(cluster)，可见这里的应用就是“物品”，集群就是“背包”，且与上文的讨论相符。

每个应用有多个不同维度的资源，这是比起传统的背包问题的复杂之处，资源可以包括CPU 资源，内存，磁盘存储，网络带宽等多个维度。易知在满足这些要求下的优化问题是 NP 完全问题。这意味着我们无法给出一个确定性的最优的答案，而是使用贪心等算法在可以接受的时间内找到一个较优解。我们甚至不能给出准确的“最优”这一概念的定义。但确定的是，解决各类背包问题的思想在我们的系统开发中都可以得到运用。

\subsubsection{总结与展望}

\subsection{效果对比}

\subsubsection{资源利用率}

\subsubsection{容灾能力}

